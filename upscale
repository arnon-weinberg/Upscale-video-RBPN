#!/usr/bin/env python3
# This script takes a directory of *.png input frame images, upscales them
# using the RBPN model pre-trained weights, and dumps the resulting frame
# images in the given output directory.
#
# >upscale --help

import argparse
import sys, os, time
import re, glob, shlex
import torch
import torch.utils.data as data
import PIL, cv2
from rbpn import Net as RBPN
from data import transform
from dataset import get_flow
from torch.autograd import Variable
import functools
print = functools.partial(print, flush=True) # Auto-flush output

parser = argparse.ArgumentParser ( description = 'Super-resolution: Upscale video using RBPN' )

parser.add_argument ( 'input', help='input video file or frames directory' )
parser.add_argument ( 'output', nargs='?', help='output frames directory' )
parser.add_argument ( '--scene',
                      metavar='[[HH:]MM:]SS[.mmm]|# -/+ [[HH:]MM:]SS[.mmm]|#',
                      help='video file time or frame range' )
parser.add_argument ( '--zoom', type=int, choices=[2,4,8], default=4,
                      help='zoom in by x2, x4, or x8 [default=4]' )
parser.add_argument ( '--frames', type=int, default=6, metavar='1-6',
                      help='context frames to use [default=6]' )
parser.add_argument ( '--no-resume', dest='resume', action='store_false',
                      help='overwrite existing output files' )
parser.add_argument ( '--no-cuda', dest='cuda', action='store_false',
                      help='use CPU even when GPU is available' )
parser.add_argument ( '--gpus', type=int, default=1, metavar='#',
                      help = 'number of GPUs to use' )
parser.add_argument ( '--threads', type=int, default=1, metavar='#',
                      help = 'number of threads to use' )
parser.add_argument ( '--testBatchSize', type=int, default=1, metavar='#',
                      help='testing batch size' )
parser.add_argument ( '--chop_forward', action='store_true' )
parser.add_argument ( '--debug', type=int, choices=range(3), default=0 )

opt = parser.parse_args()

if not 1 <= opt.frames <= 6: raise ValueError('--frames must be in range 1-6')
if opt.cuda: opt.cuda = torch.cuda.is_available()

if opt.scene:
  ts = '(?:(?:(?:\d+:)?[0-5]?\d):[0-5]?|0\d*)\d(?:\.\d*)?|(?:(?:(?:\d+:)?[0-5]?\d)?:[0-5]?|\d*)\d\.\d*' # ffmpeg time duration
  opt.scene = re.search(
      f"^(?:(?P<st>{ts})(?:-(?P<et>{ts})|\+(?P<tt>{ts})|\+(?P<vf>\d+))?" \
        +f"|(?P<sf>\d+)(?:-(?P<ef>\d+)|\+(?P<tf>\d+))?)$",
    opt.scene)
  if not opt.scene:
    raise ValueError("--scene must be a valid time or frame range")
  opt.scene = opt.scene.groupdict()

input_dir = opt.input
if   not os.path.exists ( opt.input ) or \
     ( opt.scene and not os.path.isfile ( opt.input ) ):
  raise FileNotFoundError(f"Input '{opt.input}' not found.")
elif os.path.isfile ( opt.input ):
  input_dir = os.path.join ( os.path.dirname ( opt.input ),
                             '.'+os.path.basename ( opt.input )+'.inp' )
else:
  opt.input = re.sub (r'/$', '', opt.input )

if opt.output is None:
  opt.output = os.path.join ( os.path.dirname ( opt.input ),
                              '.'+os.path.basename ( opt.input )+'.out' )
if input_dir == opt.output: opt.resume = False
if opt.debug > 1: print('opt =', opt)

if os.path.isfile ( opt.input ):
  if not opt.debug: print(f"Extracting frames from '{opt.input}' ...", end='')
  else: print(f"Extracting frames from '{opt.input}' using ffmpeg")

  command = 'ffmpeg'
  if   opt.debug == 0: command += ' -loglevel fatal -hide_banner'
  elif opt.debug == 1: command += ' -loglevel warning -hide_banner'
  if opt.resume: command += ' -n' # This doesn't work
  command += ' -i '+shlex.quote(opt.input)

  output = '%06d.png'
  if   opt.scene['st'] is not None:
    command += f" -ss {opt.scene['st']}"
    output = f"{opt.scene['st']}+%06d.png"
    if   opt.scene['et'] is not None: command += f" -to {opt.scene['et']}"
    elif opt.scene['tt'] is not None: command += f" -t {opt.scene['tt']}"
    elif opt.scene['vf'] is not None: command += f" -vframes {opt.scene['vf']}"
  elif opt.scene['sf'] is not None and opt.scene['ef'] is not None: command += f" -vf \"select='between(n,{opt.scene['sf']},{opt.scene['ef']})'\" -vsync 0 -start_number {opt.scene['sf']}"
  elif opt.scene['sf'] is not None and opt.scene['tf'] is not None: command += f" -vf \"select='between(n,{opt.scene['sf']},{int(opt.scene['sf'])+int(opt.scene['tf'])})'\" -vsync 0 -start_number {opt.scene['sf']}"
  elif opt.scene['sf'] is not None: command += f" -vf \"select='gte(n,{opt.scene['sf']})'\" -vsync 0 -start_number {opt.scene['sf']}"
  command += ' '+shlex.quote(os.path.join(input_dir,output))+' >/dev/null'

  if opt.debug > 1: print('command =', command)
  if not opt.resume and os.path.exists(input_dir):
    for f in glob.glob(os.path.join(input_dir,'*.png')): os.remove(f)
  if not os.path.exists(input_dir): os.makedirs(input_dir)
  os.system ( command )
  if not opt.debug: print(' done.')

if not os.path.exists(opt.output): os.makedirs(opt.output)

if opt.debug:
  print('Setting up model: RBPN')
else:
  print('Loading model: RBPN ...', end='')

model = RBPN ( num_channels=3, base_filter=256, feat=64, num_stages=3,
               n_resblock=5, nFrames=7, scale_factor=opt.zoom )

if opt.cuda:
  model = torch.nn.DataParallel(model, device_ids=range(opt.gpus))
else:
  model = torch.nn.DataParallel(model, device_ids=['cpu'])

pretrained_weights = os.path.join ( os.path.dirname ( sys.argv[0] ),
                                    f"weights/RBPN_{opt.zoom}x.pth" )
if opt.debug:
  print(f"  Loading pre-trained weights: file = '{pretrained_weights}'")
model.load_state_dict(torch.load ( pretrained_weights,
                                   map_location=lambda storage, loc: storage ))

if opt.cuda: model = model.cuda(0)

model.eval()
if not opt.debug: print(' done.')

# Define iterator object (ie, with __len__() and __getitem__()) for frames:
class frames_from_files(data.Dataset):
  def __init__(self, input, output, frames):
    super(frames_from_files, self).__init__()

    self.files = glob.glob(os.path.join(input,'*.png'))
    self.files.sort()
    # Preset the destination file names:
    self.dests = [os.path.join(output,os.path.basename(f)) for f in self.files]

    self.frames = frames
    self.images = {}

  def var(self, image):
    with torch.no_grad():
      if opt.cuda:
        return Variable(image).cuda(0)
      else:
        return Variable(image)

  # Cache images:
  def load_img(self, n):
    if   n >= len(self.files):
      return False
    elif n not in self.images.keys():
      self.images[n] = { 'raw': PIL.Image.open(self.files[n]).convert('RGB') }
      self.images[n]['var'] = self.var(transform()( self.images[n]['raw'] ))

    return self.images[n]

  def save_img(self, img, i):
    save_img = img.squeeze().clamp(0, 1).numpy().transpose(1,2,0)

    cv2.imwrite(self.dests[i], cv2.cvtColor(save_img*255, cv2.COLOR_BGR2RGB),
                [cv2.IMWRITE_PNG_COMPRESSION, 0])

  def __len__(self):
    return len(self.files)

  # A frame consists of an input image to upscale, neighbouring images to use
  # for context, and their corresponding flows:
  def __getitem__(self, i):
    if opt.resume and os.path.exists ( self.dests[i] ):
      if opt.debug: print(f"  Skipping: file = '{self.files[i]}'; frame #={i};"
                                   +f" destination '{self.dests[i]}' exists")
      return [ 0, 0, 0, -1 ]

    sf = i - int(self.frames/2)
    if sf < 0: sf = 0
    ef = sf + self.frames
    while not self.load_img(ef): ef -= 1
    sf = ef - self.frames
    if sf < 0: sf = 0
    ns = [n for n in range(sf,ef+1) if n!=i]
    # To simulate <6 frames, use the input image to fill in empty neighbouring
    # frames.  Note that this results in numerous "Laplacian noise" errors that
    # should be filtered out of the ouput:
    for n in range(6-len(ns)): ns.append(i)

    if opt.debug: print(f"  Loading: file = '{self.files[i]}'; frame #={i};"
                                 +f" neighbors={sf}-{ef}")
    if opt.debug > 1: print('    ns =', ns)
    input = self.load_img(i)['var'] # The image to upscale
    neighbors = [] # Neighbouring images to reference
    flows = []
    for n in ns:
      neighbors.append(self.load_img(n)['var'])
      temp = get_flow(self.load_img(i)['raw'],self.load_img(n)['raw'])
      temp = self.var(torch.from_numpy(temp.transpose(2,0,1)))
      flows.append(temp.float())

    return [ input, neighbors, flows, i ]

if opt.debug:
  print('Processing frames...')
else:
  print(f"Processing frames in '{input_dir}'", end='')

frames = frames_from_files ( input_dir, opt.output, opt.frames )
for frame in data.DataLoader ( dataset=frames, num_workers=opt.threads,
                               batch_size=opt.testBatchSize, shuffle=False ):
  i = frame.pop().numpy()[0] # Hacky way to get index...
  if i < 0:
    if not opt.debug: print('.', end='')
    continue

  if opt.debug: print(f"  Generating: file = '{frames.files[i]}'"
                      +f" -> '{frames.dests[i]}'")
  t0 = time.time()
  with torch.no_grad():
    if opt.chop_forward:
      prediction = chop_forward(*frame, model, opt.zoom)
    else:
      prediction = model(*frame)

  if opt.debug: print(f"  Saving: file = '{frames.dests[i]}'"
                      +f" took {time.time() - t0 :0.1f}s to generate")
  frames.save_img ( prediction.cpu().data, i )
  if not opt.debug: print('.', end='')
if not opt.debug: print('')

def chop_forward(x, neigbor, flow, model, scale, shave=8, min_size=2000, nGPUs=opt.gpus):
    b, c, h, w = x.size()
    h_half, w_half = h // 2, w // 2
    h_size, w_size = h_half + shave, w_half + shave
    inputlist = [
        [x[:, :, 0:h_size, 0:w_size], [j[:, :, 0:h_size, 0:w_size] for j in neigbor], [j[:, :, 0:h_size, 0:w_size] for j in flow]],
        [x[:, :, 0:h_size, (w - w_size):w], [j[:, :, 0:h_size, (w - w_size):w] for j in neigbor], [j[:, :, 0:h_size, (w - w_size):w] for j in flow]],
        [x[:, :, (h - h_size):h, 0:w_size], [j[:, :, (h - h_size):h, 0:w_size] for j in neigbor], [j[:, :, (h - h_size):h, 0:w_size] for j in flow]],
        [x[:, :, (h - h_size):h, (w - w_size):w], [j[:, :, (h - h_size):h, (w - w_size):w] for j in neigbor], [j[:, :, (h - h_size):h, (w - w_size):w] for j in flow]]]

    if w_size * h_size < min_size:
        outputlist = []
        for i in range(0, 4, nGPUs):
            with torch.no_grad():
                input_batch = inputlist[i]#torch.cat(inputlist[i:(i + nGPUs)], dim=0)
                output_batch = model(input_batch[0], input_batch[1], input_batch[2])
            outputlist.extend(output_batch.chunk(nGPUs, dim=0))
    else:
        outputlist = [
            chop_forward(patch[0], patch[1], patch[2], model, scale, shave, min_size, nGPUs) \
            for patch in inputlist]

    h, w = scale * h, scale * w
    h_half, w_half = scale * h_half, scale * w_half
    h_size, w_size = scale * h_size, scale * w_size
    shave *= scale

    with torch.no_grad():
        output = Variable(x.data.new(b, c, h, w))
    output[:, :, 0:h_half, 0:w_half] \
        = outputlist[0][:, :, 0:h_half, 0:w_half]
    output[:, :, 0:h_half, w_half:w] \
        = outputlist[1][:, :, 0:h_half, (w_size - w + w_half):w_size]
    output[:, :, h_half:h, 0:w_half] \
        = outputlist[2][:, :, (h_size - h + h_half):h_size, 0:w_half]
    output[:, :, h_half:h, w_half:w] \
        = outputlist[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]

    return output
